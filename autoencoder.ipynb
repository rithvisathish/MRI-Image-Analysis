{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7d4fc2",
   "metadata": {},
   "source": [
    "Autoencoder for Brain MRI Classification with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "569b1c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5d5f0",
   "metadata": {},
   "source": [
    "Load and Preprocess Image Data\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Set the folder path for images.  \n",
    "2. Read the labels CSV file and clean filenames.  \n",
    "3. Initialize lists for images and labels.  \n",
    "4. Loop over each row:  \n",
    "   1. Get filename and label.  \n",
    "   2. Load image.  \n",
    "   3. Resize to 64x64.  \n",
    "   4. Append to lists.  \n",
    "5. Convert to NumPy arrays.  \n",
    "6. Print shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f17d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (300, 64, 64, 3) <class 'numpy.ndarray'>\n",
      "y: (300,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"C:/Users/rithv/GitHub/brain-mri-scans/code/images/\"\n",
    "\n",
    "df = pd.read_csv(\"labels.csv\")\n",
    "df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    label = row[\"target\"]\n",
    "\n",
    "    img_path = os.path.join(folder_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        print(\"Skipping:\", img_path)\n",
    "        continue\n",
    "\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    images.append(img)\n",
    "    labels.append(float(label))\n",
    "\n",
    "x = np.array(images)   # <-- REAL NumPy array again\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"x:\", x.shape, type(x))\n",
    "print(\"y:\", y.shape, type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c202a2c",
   "metadata": {},
   "source": [
    "Convert Images to Grayscale and Print Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca54f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "x_gray = np.mean(x, axis=-1, keepdims=True)   # (300, 64, 64, 1)\n",
    "print(x_gray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3a099",
   "metadata": {},
   "source": [
    "Augment Data for Training\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Create an ImageDataGenerator for augmentation.  \n",
    "2. Generate augmented images to reach a count of 3000.  \n",
    "3. Convert to NumPy array.  \n",
    "4. Print shape.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6380e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "augmented_img = []\n",
    "count = 3000\n",
    "\n",
    "for batch in datagen.flow(x_gray, batch_size=32, shuffle=False):\n",
    "    for img in batch:\n",
    "        augmented_img.append(img)\n",
    "        if len(augmented_img) >= count:\n",
    "            break\n",
    "    if len(augmented_img) >= count:\n",
    "        break\n",
    "\n",
    "x_aug = np.array(augmented_img)\n",
    "print(x_aug.shape)   # (3000, 64, 64, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933179",
   "metadata": {},
   "source": [
    "Normalize Data and Perform Train Test Split\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Normalize grayscale images to [0,1].  \n",
    "2. Split x_gray and y into training and testing sets with 80/20 ratio.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50fd1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gray = x_gray / 255.0\n",
    "x_aug = x_aug / 255.0\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_gray, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146816ec",
   "metadata": {},
   "source": [
    "Define Autoencoder Building Function\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Define a function to build the autoencoder and encoder models.  \n",
    "2. Create input layer.  \n",
    "3. Build encoder with Conv2D and MaxPooling2D.  \n",
    "4. Build decoder with Conv2D and UpSampling2D.  \n",
    "5. Create autoencoder and encoder models.  \n",
    "6. Return both models.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25917bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape=(64, 64, 1)):\n",
    "    input_img = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    z = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    z = MaxPooling2D((2, 2), padding='same')(z)\n",
    "\n",
    "    z = Conv2D(64, (3, 3), activation='relu', padding='same')(z)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(z)\n",
    "\n",
    "    # Decoder\n",
    "    z = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    z = UpSampling2D((2, 2))(z)\n",
    "\n",
    "    z = Conv2D(32, (3, 3), activation='relu', padding='same')(z)\n",
    "    z = UpSampling2D((2, 2))(z)\n",
    "\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(z)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15154ee",
   "metadata": {},
   "source": [
    "Define Autoencoder Training Function\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Define a function to compile and train the autoencoder.  \n",
    "2. Compile with Adam optimizer and MSE loss.  \n",
    "3. Fit on augmented data with validation split.  \n",
    "4. Return the trained autoencoder.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d59fccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(autoencoder, x_aug, epochs=20, batch_size=32):\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    autoencoder.fit(x_aug, x_aug, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    return autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9e90f",
   "metadata": {},
   "source": [
    "Define Classifier Building Function for Transfer Learning\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Define a function to build the classifier using the encoder.  \n",
    "2. Freeze encoder layers.  \n",
    "3. Create Sequential model with encoder, Flatten, Dense layers.  \n",
    "4. Return the classifier.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ba4b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(encoder):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    classifier = Sequential([\n",
    "        encoder,\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b90aa5",
   "metadata": {},
   "source": [
    "Define Classifier Training Function\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Define a function to compile and train the classifier.  \n",
    "2. Compile with Adam optimizer, binary_crossentropy loss, and accuracy metric.  \n",
    "3. Fit on training data with validation split.  \n",
    "4. Return the trained classifier.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d169df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier, x_train, y_train, epochs=10, batch_size=32):\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    classifier.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329b1ee",
   "metadata": {},
   "source": [
    "Define Classifier Evaluation Function\n",
    "\n",
    "<div style=\"font-size:85%\">\n",
    "  \n",
    "1. Define a function to evaluate the classifier on test data.  \n",
    "2. Evaluate and return accuracy.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cbb1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, x_test, y_test):\n",
    "    loss, accuracy = classifier.evaluate(x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abb3c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 199ms/step - loss: 0.0370 - val_loss: 0.0247\n",
      "Epoch 2/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 198ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 3/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 4/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 5/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 6/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 199ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 7/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 8/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 201ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 9/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 202ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 10/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 11/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 12/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 192ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 13/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 192ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 14/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 191ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 15/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 16/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 17/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 189ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 18/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 196ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 19/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 205ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 20/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 215ms/step - loss: 0.0246 - val_loss: 0.0247\n",
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - accuracy: 0.4722 - loss: 2.8422 - val_accuracy: 0.5417 - val_loss: 0.9981\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.5833 - loss: 0.7589 - val_accuracy: 0.4583 - val_loss: 0.8456\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.5139 - loss: 0.7709 - val_accuracy: 0.4583 - val_loss: 0.7890\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.5139 - loss: 0.7104 - val_accuracy: 0.4583 - val_loss: 0.7137\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.5417 - loss: 0.6936 - val_accuracy: 0.5417 - val_loss: 0.6931\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.5417 - val_loss: 0.6930\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.5417 - val_loss: 0.6930\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.5417 - val_loss: 0.6930\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.5417 - val_loss: 0.6930\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.4861 - loss: 0.6932 - val_accuracy: 0.5417 - val_loss: 0.6931\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.5333 - loss: 0.6931\n",
      "Test Accuracy: 0.5333333611488342\n"
     ]
    }
   ],
   "source": [
    "#Build and train autoencoder\n",
    "autoencoder, encoder = build_autoencoder()\n",
    "autoencoder = train_autoencoder(autoencoder, x_aug)\n",
    "\n",
    "# Build and train classifier\n",
    "classifier = build_classifier(encoder)\n",
    "classifier = train_classifier(classifier, x_train, y_train)\n",
    "\n",
    "# Evaluate classifier\n",
    "test_accuracy = evaluate_classifier(classifier, x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
